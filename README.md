# Système de Détection d'Intrusions par Machine Learning

<p align="center">
  <img src="Système_de_Détection_d'Intrusi.png" alt="Schéma du projet de détection d'intrusion" width="700">
</p>

## Contexte du projet

Ce projet vise à développer un système de détection d'intrusions réseau en s'appuyant sur des techniques d'apprentissage automatique. Un système de détection d'intrusion (IDS) a pour objectif d'identifier des comportements anormaux ou malveillants au sein du trafic réseau. L'utilisation du machine learning permet d'apprendre à partir de données historiques de connexions réseau afin de classifier automatiquement de nouvelles connexions comme normales ou malveillantes (intrusion). Ce projet englobe la préparation des données, la construction d'un pipeline de classification supervisée, l'entraînement de plusieurs modèles de machine learning et l'évaluation de leurs performances pour identifier les modèles les plus efficaces et robustes pour la détection d'intrusions.

## Jeu de données

Le jeu de données utilisé, appelé ici *Device Dataset*, est issu du domaine de la sécurité réseau et contient des enregistrements de trafic réseau étiquetés soit comme *normal* soit comme *intrusion* (de différentes natures). Il comprend des caractéristiques numériques (comptages, taux, octets échangés, etc.) ainsi que des caractéristiques catégorielles (par ex. type de protocole, service réseau sollicité). Au total, après préparation initiale, le dataset compte environ **15 829 échantillons** et 80 features (caractéristiques) une fois les variables catégorielles encodées. La variable cible s'appelle `outcome` et indique le type de connexion (normal ou type d'attaque). On note que la classification est multi-classe (une classe *normal* et plusieurs classes d'attaques), avec un déséquilibre : environ 56,4% des connexions sont normales, les autres se répartissant en plusieurs classes d'attaques plus ou moins rares. **À noter qu'un second test a été réalisé ultérieurement sur la **totalité** du jeu de données (environ 170 000 enregistrements) afin d'évaluer la performance des modèles sur un ensemble bien plus large** (voir sections plus bas pour les résultats comparatifs).

**Analyse exploratoire et qualité des données :** Le profilage du jeu de données a mis en évidence plusieurs points importants concernant sa qualité et sa structure :

* **Colonnes constantes** : Certaines features sont constantes sur toutes les lignes du dataset (par exemple, `urgent` et `num_outbound_cmds` valent toujours 0). Ces colonnes n'apportent aucune information discriminante et ont été identifiées pour suppression.
* **Variables déséquilibrées** : Plusieurs features catégorielles sont extrêmement dominées par une seule valeur (plus de 90% des lignes pour une même catégorie), ce qui réduit leur intérêt prédictif. Par exemple, les features comme `land`, `wrong_fragment`, `num_failed_logins`, `root_shell`, `su_attempted`, `num_shells` ou `is_guest_login` prennent une valeur unique dans plus de 90% des cas. Ce manque de variance limite leur utilité pour la classification.
* **Distribution asymétrique (skewness)** : Certaines variables numériques présentent une forte asymétrie avec des valeurs extrêmes. Par exemple, `src_bytes` et `dst_bytes` ont une distribution très étalée (avec des pics de plus d'un milliard d'octets pour `dst_bytes`), tout comme `num_compromised` ou `num_root` qui sont très *skewed*. Ces distributions asymétriques peuvent nécessiter des transformations ou une mise à l'échelle adaptée pour les modèles sensibles à l'échelle des données.
* **Grand nombre de zéros** : De nombreuses variables numériques contiennent une proportion élevée de zéros. Par exemple, `duration` est nulle dans \~92% des connexions, `hot` \~98%, `num_compromised` \~99%, `num_root` \~99%, `num_file_creations` \~99.7%, `num_access_files` \~99.8%, etc. Cette prédominance de valeurs nulles peut biaiser certains algorithmes ou les métriques si elle n'est pas prise en compte.
* **Doublons** : Environ 1 856 lignes dupliquées (soit \~9,8% du dataset) ont été détectées. Ces doublons risquent d’influencer l’entraînement en sur-représentant certains exemples et ont donc été supprimés lors du prétraitement.
* **Valeurs extrêmes** : Outre la skewness mentionnée, des *outliers* très marqués sont présents (notamment sur `src_bytes` et `dst_bytes`). Ils peuvent perturber certains modèles (comme KNN ou SVM) qui utilisent des distances, et motivent l'utilisation d'une normalisation des features.
* **Déséquilibre de la variable cible** : La classe majoritaire (*normal*) représente 56,4% des exemples, les intrusions constituant le reste. Ce déséquilibre modéré impose de regarder au-delà de la simple accuracy (précision globale) lors de l'évaluation, en se focalisant aussi sur des métriques comme le rappel ou le F1-score pour s'assurer que les attaques (classes minoritaires) sont bien détectées.
* **Types de variables variés** : Le dataset comporte à la fois des variables numériques continues, des variables catégorielles (telles que le protocole réseau ou le service) et même des identifiants textuels. Cela nécessite des traitements de préprocessing appropriés pour chaque type (encodage des catégories, normalisation des numériques, etc.) avant de pouvoir entraîner les modèles.

## Prétraitement des données

Pour assurer la qualité des données en entrée des modèles, plusieurs étapes de prétraitement ont été appliquées :

* **Nettoyage initial et filtrage des features** : Les colonnes identifiées comme non informatives ont été supprimées. En particulier, toutes les features constantes ou quasi-constantes (variance nulle ou quasi nulle) ont été éliminées. De même, les features catégorielles extrêmement déséquilibrées (une seule modalité dominante) ont été envisagées pour retrait, afin de ne pas introduire de bruit inutile.
* **Encodage des variables catégorielles** : Les features catégorielles restantes (`protocol_type`, `service`, etc.) ont été converties en variables numériques par one-hot encoding (variables binaires indicatrices) à l'aide de `OneHotEncoder` de scikit-learn. Afin d'éviter la multicolinéarité, l'option `drop='first'` a été utilisée pour chaque ensemble de *dummy variables*. Après encodage, le nombre de features est passé de 16 (features initiales hors cible) à \~80 colonnes. *(NB : La feature cible `outcome` étant multiclasse, elle a également été encodée en vecteur d'indicateurs pour certaines étapes, même si pour l'entraînement des modèles de classification on l'a conservée sous forme de label multi-classe.)*
* **Gestion des valeurs manquantes** : Une fois les données encodées, une analyse a révélé la présence de valeurs manquantes sur certaines features numériques. Ces valeurs manquantes ont été imputées en utilisant un `SimpleImputer` (`strategy='median'`) de scikit-learn, afin de remplacer les `NaN` par une valeur neutre représentative de la distribution de chaque variable. Cette imputation par la médiane est robuste face aux outliers des distributions asymétriques.
* **Suppression des doublons** : Comme mentionné, les lignes dupliquées ont été éliminées (`df.drop_duplicates()`), réduisant le risque de biais dû à la redondance de certains exemples.
* **Séparation des données d'entraînement et de test** : Le dataset nettoyé a été divisé en deux ensembles via `train_test_split` (70% des données pour l'entraînement et 30% pour le test, avec `random_state=42` pour la reproductibilité). Cette séparation garantit qu'on évalue les modèles sur des données qu'ils n'ont pas vues pendant l'entraînement, pour mesurer leur capacité de généralisation.
* **Normalisation** : Les features numériques ont été normalisées à l'aide de `StandardScaler`. Cette étape transforme chaque feature pour qu'elle ait une moyenne de 0 et un écart-type de 1 (calculés sur l'ensemble d'entraînement, puis appliqués à l'ensemble de test) afin d'harmoniser les échelles et d’éviter que des variables à grande amplitude ne dominent le calcul des distances ou gradients dans les modèles.
* **Réduction de dimension (Feature Selection)** : Afin de simplifier le modèle et d'éliminer les features peu utiles, un filtre de variance a été appliqué sur les features encodées. En utilisant `VarianceThreshold(threshold=0.1)`, les features dont la variance (sur l'ensemble d'entraînement) est inférieure à 0,1 ont été retirées. Ce processus a réduit le nombre de features à environ 14 dimensions les plus informatives, ce qui allège la complexité des modèles (utile notamment pour KNN, SVM, MLP) sans trop sacrifier l'information.

## Modèles de machine learning entraînés

Plusieurs algorithmes d'apprentissage supervisé ont été entraînés et comparés pour la tâche de classification d'intrusions. Chaque modèle a été entraîné sur le set d'entraînement issu du jeu de données réduit (\~15k échantillons) après prétraitement, en utilisant les paramètres par défaut de scikit-learn (sauf mention contraire). Les modèles considérés couvrent différentes familles d'algorithmes afin de diversifier les approches :

* **K-Nearest Neighbors (KNN)** – *Méthode à base d'instances* : classifie un échantillon en fonction des classes de ses voisins les plus proches dans l'espace des features. Un KNN avec *k=5* a été utilisé, précédé d'une normalisation des données pour que les distances ne soient pas dominées par des échelles différentes. KNN a l'avantage d'être simple et non paramétrique (pas de phase d'entraînement lourde), mais son coût de prédiction est élevé (il faut parcourir la base d'entraînement pour chaque nouvelle instance) et sa performance peut chuter si la dimensionnalité est élevée ou les données bruitées.
* **Multi-Layer Perceptron (MLP)** – *Réseau de neurones artificiels* : un perceptron multi-couches avec une couche cachée de 100 neurones (architecture simple) a été entraîné. Le MLP peut capturer des relations non linéaires complexes grâce à ses neurones cachés. Il utilise l'algorithme de rétropropagation pour ajuster ses poids. Nous avons limité le nombre d'itérations à 200 (paramètre par défaut) pour l'entraînement. Les réseaux de neurones peuvent potentiellement bien s'adapter à des données complexes, mais ils sont sensibles aux hyperparamètres (architecture, taux d'apprentissage, etc.) et nécessitent généralement un volume de données important pour s'entraîner correctement sans surajustement.
* **Decision Tree** – *Arbre de décision* : un arbre de décision de classification a été entraîné (critère "gini") en limitant sa profondeur maximale à 5 niveaux pour éviter un sur-ajustement complet. Les arbres de décision apprennent des règles sous forme de divisions récursives de l'espace des features et sont bien adaptés aux données hétérogènes (mélange de numériques et catégorielles) sans nécessiter de mise à l'échelle. Ils offrent de la lisibilité (on peut interpréter le chemin de décision), mais seuls, ils peuvent sur-ajuster si trop profonds.
* **Random Forest (Forêt Aléatoire)** – Ensemble de 100 arbres de décision entraînés sur des sous-échantillons aléatoires du dataset (méthode du *bagging*). Chaque arbre a une profondeur max de 20 également, pour contrôler la complexité. La forêt aléatoire agrège les prédictions de multiples arbres (vote majoritaire) pour améliorer la robustesse et la généralisation. Ce modèle gère bien les outliers et les variables bruitées, et a souvent de très bonnes performances par défaut grâce à la réduction de variance qu'apporte l'ensemble.
* **Gradient Boosting** – Ensemble d'arbres de décision entraînés de manière séquentielle (*boosting*) pour corriger les erreurs des arbres précédents. Nous avons utilisé un `GradientBoostingClassifier` avec 100 estimators, un taux d'apprentissage de 0.1 et profondeur d'arbres de 3 (paramètres classiques). Le Gradient Boosting excelle souvent sur des données aux patterns complexes et peut mieux gérer le déséquilibre de classes en donnant plus de poids aux erreurs, au prix d'un temps de calcul plus élevé. C'est un modèle puissant qui peut obtenir des performances élevées en optimisant finement les hyperparamètres.
* **Support Vector Machine (SVM)** – Classifieur à vecteurs de support utilisant un noyau gaussien (RBF) avec paramètre de régularisation C=1. Les SVM cherchent à maximiser la marge entre classes et peuvent modéliser des frontières non linéaires via le *kernel trick*. Ils sont efficaces en classification binaire et multi-classe et théoriquement performants en haute dimension. Toutefois, ils peuvent être sensibles aux données bruitées et le temps de calcul/mémoire peut être important sur de grands jeux de données. Ici, un SVM avec noyau RBF standard a été testé.

Pour chaque modèle, nous avons utilisé scikit-learn pour l'entraînement (`.fit`) et la prédiction sur les données de test (`.predict`). Aucun réglage d'hyperparamètres approfondi n'a été initialement effectué (les valeurs par défaut ou usuellement bonnes ont été utilisées comme indiqué ci-dessus). Toutefois, une phase d'optimisation des hyperparamètres a été envisagée en second lieu à l'aide de `RandomizedSearchCV` (validation croisée sur grille aléatoire) afin d'explorer des combinaisons de paramètres pour chaque algorithme. Cette recherche d'hyperparamètres, effectuée pour un sous-ensemble de modèles (KNN, MLP, Decision Tree, Random Forest, Gradient Boosting, SVM), visait à potentiellement améliorer encore les performances. Les résultats présentés ci-dessous sont basés sur les modèles de base avant tuning, à moins d'indication contraire.

## Évaluation des modèles et résultats

L'évaluation des performances a été réalisée sur l'ensemble de test mis de côté du jeu de données réduit (\~30% des 15 829 échantillons), en calculant plusieurs métriques de classification : **accuracy** (taux de bonne classification global), **précision moyenne pondérée**, **rappel moyen pondéré** et **F1-score** (moyenne harmonique précision/rappel). Ces métriques sont calculées de façon macro-agrégée sur l'ensemble des classes afin de tenir compte du caractère multi-classe et du déséquilibre. En particulier, la précision et le rappel nous informent sur la capacité à éviter respectivement les faux positifs et les faux négatifs, ce qui est crucial en détection d'intrusion (mieux vaut détecter toutes les intrusions, tout en minimisant les fausses alertes).

Les performances obtenues pour chaque modèle sur le jeu de données réduit sont résumées dans le tableau ci-dessous (les valeurs en **gras** indiquent la meilleure performance pour chaque métrique) :

| Modèle                 |   Accuracy |  Précision |     Rappel |   F1-Score |
| ---------------------- | ---------: | ---------: | ---------: | ---------: |
| K-Nearest Neighbors    |     0.9800 |     0.9801 |     0.9800 |     0.9800 |
| Multi-Layer Perceptron |     0.9817 |     0.9818 |     0.9817 |     0.9817 |
| Decision Tree          |     0.9861 |     0.9862 |     0.9861 |     0.9861 |
| Support Vector Machine |     0.9667 |     0.9668 |     0.9667 |     0.9667 |
| **Random Forest**      | **0.9909** | **0.9910** | **0.9909** | **0.9909** |
| Gradient Boosting      |     0.9884 |     0.9885 |     0.9884 |     0.9884 |

**Observations :** Tous les modèles atteignent des performances élevées sur ce jeu de données réduit, avec une accuracy supérieure à \~96% dans tous les cas. Néanmoins, des différences subsistent entre les algorithmes :

* **Random Forest** obtient les meilleurs résultats globaux avec \~99.1% d'accuracy et un F1-score de 0.9909. C'est le modèle qui semble le plus efficace pour ce problème, ce que l'on peut attribuer à sa capacité d'ensemble (*bagging*) à réduire le sur-ajustement tout en capturant la plupart des patterns du dataset. Il gère bien les variables bruyantes ou redondantes et est peu affecté par les outliers ou l'asymétrie des distributions.
* **Gradient Boosting** s'approche de très près des performances de Random Forest (\~98.84% d'accuracy). Il parvient également à très bien capturer les interactions complexes entre features et à gérer le déséquilibre des classes en mettant plus l'accent sur les erreurs. Sa légère sous-performance par rapport à Random Forest pourrait être due au fait qu'il est un peu plus susceptible de sur-apprendre sur certaines classes rares ou qu'il n'a pas été poussé à un nombre d'arbres suffisant. Néanmoins, il demeure l'un des top modèles en termes de précision et rappel.
* **Arbre de Décision** seul obtient un excellent score (\~98.6% d'accuracy), ce qui est notable compte tenu de sa simplicité. En limitant sa profondeur, on a évité qu'il sur-ajuste complètement les données d'entraînement, et il a su capturer les principaux critères de décision du dataset. Il offre un bon compromis performance/simplicité, même s'il reste en deçà des méthodes d'ensemble en performance brute.
* **Multi-Layer Perceptron (MLP)** affiche environ 98.17% d'accuracy, légèrement mieux que le KNN. Le réseau de neurones a su modéliser des relations non linéaires complexes, ce qui explique qu'il surpasse le KNN qui est plus "local". Cependant, son apprentissage a pu être influencé par le déséquilibre de classes (bien que cela ait été atténué par l'usage de métriques pondérées) et il nécessiterait potentiellement plus de réglages (architecture, epochs, etc.) pour atteindre son plein potentiel.
* **K-Nearest Neighbors (KNN)** atteint \~98.0% d'accuracy, ce qui est très honorable. Ses scores de précision et rappel montrent qu'il classe correctement la plupart des exemples. La normalisation des données a clairement contribué à cette bonne performance en évitant qu'une feature à grande échelle ne domine le calcul de distance. KNN reste toutefois sensible aux points aberrants et peut voir ses performances chuter si le bruit augmente ou si des features non pertinentes ne sont pas filtrées. Dans notre pipeline, l'élimination des features non discriminantes et la réduction de dimension ont probablement aidé KNN à se concentrer sur les dimensions les plus utiles.
* **Support Vector Machine (SVM)** obtient les scores les plus faibles du panel (\~96.7% d'accuracy). Bien qu'encore élevés, ces résultats inférieurs aux autres modèles suggèrent que le SVM a peut-être été perturbé par le bruit ou par la structure complexe du dataset. Un SVM avec noyau RBF peut avoir du mal si les classes ne sont pas bien séparées avec une marge claire ou s'il y a beaucoup d'outliers. De plus, sans ajustement fin des hyperparamètres (comme le paramètre C ou un noyau différent), le SVM peut être moins performant que les modèles d'ensemble. Il est possible qu'une optimisation (par ex. tester un noyau polynomial ou ajuster C/γ) améliore ses performances. Ici, il se classe en dernier principalement à cause de la très forte performance des autres modèles.

**Comparaison avec le jeu de données complet :** Bien que les performances ci-dessus soient excellentes sur le jeu de données *réduit*, une évaluation supplémentaire a été menée sur l'ensemble **complet** (environ 170k connexions) pour vérifier la robustesse des modèles. Sans ré-entraînement sur ces nouvelles données, on observe une **baisse généralisée des métriques** pour tous les algorithmes. L'ampleur de cette baisse varie toutefois fortement selon les modèles : les méthodes d'ensemble (**Random Forest**, **Gradient Boosting**) ainsi que l'**Arbre de décision** se montrent relativement **robustes**, ne perdant qu'environ **4 à 5 points** de pourcentage d'accuracy sur le grand jeu de données, tandis que le **MLP** et le **KNN** subissent des chutes beaucoup plus prononcées (de l'ordre de **15 à 18 points**). Le **SVM**, de son côté, connaît une baisse intermédiaire (environ **12 points** d'accuracy en moins). Ces résultats indiquent que sur un ensemble de données plus vaste et potentiellement plus hétérogène, les modèles comme Random Forest ou Gradient Boosting conservent des performances élevées (aux alentours de 93–94% d'accuracy), alors que d'autres modèles plus simples ou plus sensibles voient leur efficacité diminuer nettement. Les modèles d'ensemble et l'arbre semblent avoir mieux capturé des patterns généralisables du problème dans le petit jeu de données, ce qui leur permet de bien s'adapter aux nouvelles observations, là où le KNN (basé sur des instances locales) et le MLP (réseau de neurones potentiellement sous-entraîné) peinent à retrouver les mêmes performances sans avoir été entraînés sur davantage de données. On constate ainsi l'importance de tester les algorithmes sur des volumes de données plus larges pour évaluer leur **capacité de généralisation** au-delà de l'échantillon initial.

## Analyse critique et conclusions

Les constats ci-dessus montrent que les méthodes d'ensemble se démarquent pour ce problème de détection d'intrusions. En particulier, le Random Forest est apparu comme le modèle le plus robuste et performant sur l'ensemble de test *réduit*, suivi de près par le Gradient Boosting. Ces modèles combinent les prédictions de multiples apprenants faibles (arbres) et réussissent ainsi à capturer la majorité des motifs complexes tout en lissant les anomalies du jeu de données. Ils tolèrent mieux les variations dans les données (bruit, outliers, variables peu informatives) et ont moins tendance à sur-apprendre grâce à la moyenne des contributeurs (RF) ou à la régularisation implicite du boosting.

Les modèles plus simples comme l'Arbre de décision et le KNN ont surpris par leurs bonnes performances une fois les données correctement préparées. Un arbre de décision bien paramétré peut suffire à obtenir un niveau de précision élevé, et il a l'avantage d'être interprétable. Le KNN, malgré sa simplicité, profite grandement de la normalisation et de la réduction de dimension effectuées : sans features inutiles et avec des distances équilibrées, il parvient à classer correctement une très grande partie des connexions. Cependant, comme anticipé, ces modèles restent sensibles aux variations des données : par exemple, l'ajout de bruit ou l'arrivée de nouvelles attaques non représentées dans l'échantillon d'entraînement pourrait les dégrader plus fortement qu'un ensemble plus sophistiqué.

Le Réseau de neurones (MLP) offre de bonnes performances et a le potentiel de s'améliorer encore via l'ajustement de son architecture ou de son entraînement. Il peut capturer des patterns complexes que d'autres modèles pourraient manquer. Cependant, il nécessite beaucoup de données pour être entraîné de façon optimale et peut être affecté par un léger déséquilibre des classes. Dans notre cas, il n'a pas surpassé les méthodes d'ensemble, probablement car le dataset réduit, bien que complexe, était géré efficacement par les arbres et forêts une fois les bons features sélectionnés. L'évaluation sur le jeu complet a par ailleurs révélé que sans plus de données d'entraînement, ce modèle était l'un de ceux dont la performance chutait le plus face à la diversité accrue des exemples.

Enfin, le SVM s'est montré un peu en retrait sur le jeu réduit. Son usage pourrait nécessiter une meilleure calibration pour rivaliser avec les autres (par ex. utiliser une recherche en grille pour le paramètre de régularisation ou tester un noyau différent). Sa performance montre que, sans tuning approfondi, il est dépassé par des méthodes plus flexibles ou collectives sur ce dataset. Néanmoins, il n'a pas non plus déclenché d'alertes catastrophiques : tous les modèles ont fourni une détection d'intrusion satisfaisante sur le petit jeu de données, avec des F1-scores pondérés aux alentours de 0.98 ou plus (ce qui signifie que très peu d'attaques légitimes n'ont pas été détectées – faible taux de faux négatifs – et peu de trafic normal a été incorrectement étiqueté comme malveillant – faible taux de faux positifs). Sur le jeu complet, le SVM a vu ses performances diminuer comme indiqué plus haut, soulignant qu'il pourrait bénéficier d'un retrain sur plus de données ou d'un ajustement de paramètres pour mieux capturer la structure globale.

En conclusion, cette étude met en avant l'importance d'un bon prétraitement (nettoyage, encodage, normalisation, réduction de dimension) qui a grandement contribué à la performance élevée de tous les modèles sur les données initiales. Parmi les algorithmes testés, les modèles d'ensemble (Random Forest en tête) se distinguent par leur robustesse face aux variations des données et offrent les meilleures garanties de généralisation. Pour une utilisation en production dans un IDS, on privilégierait donc un de ces modèles, en l'accompagnant éventuellement d'un affinement des hyperparamètres et d'une surveillance continue des performances (surtout si de nouvelles formes d'attaques apparaissent, nécessitant une mise à jour du modèle). L'analyse comparative sur petit vs grand jeu de données a également souligné qu'il est crucial de valider les modèles sur des ensembles plus larges : certains algorithmes peuvent donner une illusion de performance sur un échantillon restreint mais voir leur efficacité réelle diminuer à l'échelle, d'où l'importance de tester en conditions plus réalistes avant déploiement.

## **Analyse comparative entre petit et grand jeu de données**

Dans cette section, nous comparons plus en détail les résultats obtenus sur le **jeu de données réduit (\~15,8k échantillons)** et sur le **jeu de données complet (\~170k échantillons)**, afin de mettre en lumière les écarts de performances et d'analyser quels modèles résistent le mieux à l'augmentation de la taille et de la diversité des données.

**Résumé des performances (Accuracy) :** Le tableau ci-dessous récapitule l'accuracy de chaque modèle évalué sur le jeu réduit (test de 30% des 15k) versus sur le jeu complet (l'ensemble complet servant ici de test pour des modèles entraînés sur le jeu réduit) :

| Modèle                 |   Accuracy (jeu 15k) | Accuracy (jeu 170k) |
| ---------------------- | -------------------: | ------------------: |
| **Random Forest**      | **0.9909** (\~99.1%) |    0.9413 (\~94.1%) |
| **Gradient Boosting**  |     0.9884 (\~98.8%) |    0.9327 (\~93.3%) |
| **Decision Tree**      |     0.9861 (\~98.6%) |    0.9390 (\~93.9%) |
| Support Vector Machine |     0.9667 (\~96.7%) |    0.8477 (\~84.8%) |
| K-Nearest Neighbors    |     0.9800 (\~98.0%) |    0.8256 (\~82.6%) |
| Multi-Layer Perceptron |     0.9817 (\~98.2%) |    0.8014 (\~80.1%) |

On observe que les **Random Forest, Gradient Boosting et l'Arbre de décision** restent les meilleurs sur le jeu complet avec des accuracies autour de 93–94%, malgré une légère baisse par rapport au jeu réduit (perte d’environ 5 points). Ces trois modèles semblent donc **les plus robustes** face à l'augmentation de la taille du dataset. À l'inverse, le **KNN** et le **MLP** subissent les baisses les plus sévères (respectivement \~15 et \~18 points de moins en accuracy), suivis par le **SVM** (\~12 points de moins).

**Interprétation des écarts :**

* Les **méthodes d'ensemble (RF, GB)** et l'**arbre de décision** semblent avoir capté des règles et motifs généraux du problème dès l'entraînement sur l'échantillon réduit, d'où leur relative résistance lorsque de nombreux nouveaux exemples sont introduits. Leur approche (agrégation de nombreux estimateurs pour les ensembles, hiérarchie de décisions pour l'arbre) les rend moins sensibles aux variations individuelles de données : ils généralisent mieux et ne se sont pas sur-ajustés aux particularités du petit échantillon. Par conséquent, lorsqu'on évalue ces modèles sur un volume beaucoup plus grand, leurs prédictions restent cohérentes et précises pour la plupart des nouvelles connexions, même sans les avoir ré-entraînés sur ces données supplémentaires.

* En revanche, **KNN** et **MLP** montrent une nette diminution de performance. Pour le KNN, cela s'explique par sa nature *mémoire* : entraîné sur \~11k instances (70% de 15k), il peut échouer à classer correctement de nombreuses connexions du jeu complet qui ne ressemblent pas suffisamment à un voisin connu. Plus le volume de données test est grand, plus il y a de chances de rencontrer des points atypiques ou moins denses autour desquels le KNN, limité par son petit ensemble d'apprentissage, prend une décision erronée. De plus, l'introduction de nouvelles variations ou de bruit peut particulièrement dégrader KNN, comme on l'avait anticipé, puisqu'il n'a pas de mécanisme d'abstraction : chaque point de test est comparé brute-force aux points d'entraînement, dont la couverture de l'espace était forcément limitée sur un petit jeu.

  Pour le MLP, la chute de \~98% à \~80% suggère qu'il n'a pas pu *généraliser* correctement au-delà de l'échantillon d'entraînement restreint. Plusieurs facteurs peuvent expliquer cela : (1) le réseau de neurones a potentiellement sur-appris des motifs spécifiques au petit dataset et manque de capacité (ou de régularisation) pour s'adapter à des cas qu'il n'a jamais vus ; (2) le MLP utilisé (une seule couche cachée de 100 neurones, entraîné en quelques epochs) n'a peut-être pas une architecture suffisamment complexe pour capturer toutes les variations du jeu complet, ou au contraire, il aurait nécessité plus de données d'entraînement pour ajuster correctement ses poids sans tomber dans des minima locaux inadéquats ; (3) l'arrivée massive de nouvelles connexions, potentiellement avec de subtiles différences de distribution, a mis en évidence la fragilité d'un modèle non recalibré sur ces données. En somme, le MLP avait besoin de davantage de données d'entraînement et d'un tuning plus poussé pour maintenir son niveau de performance à l'échelle du jeu complet.

* Le **SVM**, quant à lui, subit une dégradation intermédiaire. Son accuracy passe d'environ 96.7% à 84.8%. Ce recul important confirme que le modèle SVM RBF entraîné sur l'échantillon initial n'a pas pu capturer correctement la frontière de décision englobant tous les nouveaux exemples. Cependant, le SVM ne s'effondre pas autant que le KNN ou le MLP, possiblement parce que la marge maximisée lors de son apprentissage sur le jeu réduit offrait une certaine tolérance à de nouveaux points (il a appris une frontière un peu plus générale qu'un modèle purement instance-based comme KNN). Néanmoins, sans incorporation des données supplémentaires dans son entraînement, ses vecteurs de support initiaux sont insuffisants pour bien délimiter les classes dans l'univers étendu du jeu complet. Un recalibrage du SVM (par exemple via un nouvel entraînement incluant des échantillons du jeu complet, ou via un tuning d'hyperparamètres C/γ) serait nécessaire pour retrouver une meilleure performance.

**Conclusion comparative :** Cette analyse comparative met en évidence que **la hiérarchie des modèles change peu** (les mêmes modèles restent globalement en tête), mais **l'écart entre eux se creuse** sur des données plus étendues. Les modèles qui étaient déjà performants et robustes sur l'échantillon réduit (ensembles d'arbres notamment) confirment leur solidité, tandis que ceux qui étaient à la traîne ou justes en performance souffrent encore davantage à l'échelle. En pratique, cela suggère qu'un **Random Forest** ou un **Gradient Boosting** offriraient non seulement de très bonnes performances sur un petit jeu de données, mais conserveraient une efficacité élevée en production sur de larges flux de trafic, là où un KNN ou un MLP conçus sur un petit set pourraient nécessiter une ré-architecture ou un nouvel entraînement sur davantage de données pour ne pas voir leur taux d'erreur exploser.

Il convient de noter que dans un contexte réaliste, on aurait retrainé les modèles sur l'ensemble complet ou un échantillon plus représentatif, ce qui aurait probablement amélioré les scores de certains algorithmes (notamment le MLP qui aurait bénéficié de plus de données pour apprendre, ou le KNN dont l'efficacité augmente avec plus de voisins connus). Notre démarche ici visait à simuler une situation de *généralisation* : les modèles entraînés sur un sous-ensemble ont été testés tels quels sur un volume massif de nouvelles données. Cet exercice a permis d'identifier lesquels possèdent une bonne capacité de généralisation intrinsèque (Random Forest, Gradient Boosting, Decision Tree) et lesquels sont plus fragiles sans nouvel entraînement (KNN, MLP, SVM). En résumé, tester sur un jeu de données plus large a renforcé la confiance dans les modèles d'ensemble pour la détection d'intrusions à grande échelle, tout en montrant les limites des modèles plus simples ou exigeants en données.

## Outils et bibliothèques utilisés

Ce projet a été réalisé en Python dans un environnement Jupyter Notebook, en s'appuyant principalement sur les bibliothèques suivantes :

* **pandas** : pour la manipulation des données (chargement du fichier CSV, opérations de nettoyage, concaténation des features encodées, etc.).
* **NumPy** : pour les opérations numériques bas niveau et la gestion de matrices de données (par exemple, identification des colonnes numériques, conversions de types, etc.).
* **YData Profiling** (anciennement *pandas-profiling*) : pour générer un rapport de profilage exploratoire du dataset, qui a fourni les statistiques descriptives utilisées dans l'analyse de la qualité des données (distribution des features, détection des constantes, doublons, valeurs manquantes, etc.).
* **scikit-learn** : bibliothèque centrale pour le machine learning, utilisée à toutes les étapes du pipeline :

  * *Prétraitement* : `OneHotEncoder` pour l'encodage des variables catégorielles, `SimpleImputer` pour l'imputation des valeurs manquantes, `StandardScaler` pour la normalisation des features.
  * *Sélection de features* : `VarianceThreshold` pour filtrer les features peu informatives.
  * *Séparation des données* : `train_test_split` pour découper le dataset en ensembles d'entraînement et de test.
  * *Modélisation* : implémentations des algorithmes de classification – `KNeighborsClassifier`, `MLPClassifier`, `DecisionTreeClassifier`, `RandomForestClassifier`, `GradientBoostingClassifier`, `SVC` (Support Vector Classifier) – avec réglage des hyperparamètres de base.
  * *Évaluation* : fonctions de métriques (`accuracy_score`, `precision_score`, `recall_score`, `f1_score`) et outils pour générer des rapports de classification ou des matrices de confusion (`classification_report`, `confusion_matrix`). *(Des visualisations avec Matplotlib/Seaborn des matrices de confusion ou des distributions auraient pu être réalisées, bien que non systématiquement incluses dans ce notebook.)*
  * *Optimisation* : `RandomizedSearchCV` pour la recherche aléatoire d'hyperparamètres avec validation croisée, utilisée en fin de projet pour explorer l'amélioration potentielle des modèles.
* **Matplotlib & Seaborn** : bibliothèques de visualisation utilisées à des fins exploratoires (par exemple, tentative d'afficher la matrice des features normalisées via un *heatmap*, histogrammes de distribution, etc.). Elles permettent de mieux comprendre les données et auraient pu servir à visualiser les performances (ex: courbes ROC, importances des features pour la Random Forest, etc.).

L'utilisation conjointe de ces outils a permis de construire un pipeline de machine learning complet, de l'analyse initiale du jeu de données jusqu'à l'évaluation finale des algorithmes de classification.
